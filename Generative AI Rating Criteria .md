Project: AAIE – Artificial Assessment Intelligence for Education

Week 6: Generative AI Rating Criteria Task

Activity: Model Development. [Planner - Generative AI Rating Criteria](https://teams.microsoft.com/l/entity/com.microsoft.teamspace.tab.planner/mytasks?tenantId=d02378ec-1688-46d5-8540-1c28b5f470f6&webUrl=https%3A%2F%2Ftasks.teams.microsoft.com%2Fteamsui%2FpersonalApp%2Falltasklists&context=%7B%22subEntityId%22%3A%22%2Fv1%2Fassignedtome%2Fview%2Fboard%2Ftask%2FGm-KKFL6cECuk7MdRUObD8gAL6Ck%22%7D)

**Objective:** 

- AI rating prompts for feedback generation task
- AI rating prompts for AI detection task
- Documentation on prompt testing and refinement

**Implementing:**

1. ***Hypothesis for Test Prompts:***

   - Primary Hypothesis: if the prompts are structured with explicit evaluation criteria, output formats, and reasoning requirements, then the generative AI will produce ratings that are meaningful and consistent

   - Meaningfulness (Validity): using metrics as Pearson/Spearman correlation, and Cohen's kappa agreement.
     - Threshold: Correlation >= 0.7 with human annotations.
   - Consistency (Reliability): run identical or paraphrased model outputs multiple times. 
     - Ratings should remain stable (variance <= 0.5 on a 1-5 scale).

1. ***AI Rating Prompts for Feedback Generation Task (Rating + Reasoning):***

   - Prompt evaluation should include explanation: using LLM-based judge that not only rates outputs (pass/fail or scalar rating) but also provides reasoning based on evaluation criteria which gives insight into why a response passes or fails.

   - Evaluation Criteria: rate a student-feedback response produced by an AI model, based on evaluation criteria.
   - Format:
     - Rating (Integer): score from 1 to 5 (from poor/not relevant to excellent/very relevant)
     - Reasoning:
   - Correctness – Is the feedback accurate and helpful?
   - Clarity – Is it easy to understand?
   - Tone – Is it supportive and constructive?
   - Actionability – Does it suggest a clear next step?

(3) AI Feedback Generation

\- Example:

|<p># Rate the relevance of the following response to the prompt: </p><p>Your Task: "Score this response from 1 (not relevant) to 5 (very relevant) and provide reasons for your score." </p><p></p><p>- Prompt1 = "What are the benefits of regular exercise?"</p><p>- Response = "Regular exercise improves cardiovascular health, boosts mood, and enhances overall fitness." </p><p></p><p># Evaluate clarity instead: </p><p>- Prompt2 = "How clear is this explanation? Score it from 1 (very unclear) to 5 (very clear)." </p><p>- Response = "Exercise helps you stay healthy by making your heart stronger and improving your mood."</p>|
| :- |

\- Expected Output: JSON format:

|<p>{</p><p>`  `"Rating": "<integer 1-5>",</p><p>`  `"Criterion\_Ratings": {</p><p>`    `"Correctness": "<1-5>",</p><p>`    `"Clarity": "<1-5>",</p><p>`    `"Tone": "<1-5>",</p><p>`    `"Actionability": "<1-5>"</p><p>},</p><p>`  `"Reasoning": {</p><p>`    `"Correctness": "Feedback correctly identified the logical error in the code.",</p><p>`    `"Clarity": "The explanation was clear and easy to follow.",</p><p>`    `"Tone": "Supportive and encouraging tone was maintained.",</p><p>`    `"Actionability": "Concrete next steps were suggested to fix the bug."</p><p>`  `}}</p>|
| :- |

\- References: Bakst W (03 March 2025), “Prompt Evaluation – Method, Tools, and Best Practices”. <https://mirascope.com/blog/prompt-evaluation>


1. ***AI Rating Prompts for AI Detection Task (Binary Output)***

   - Determine whether the following text was generated by AI or Human Written

   - Label (Output): AI or Human
   - Reasoning (Explanation): provide a brief reasoning (short sentence) such as cites, stylistic clues, or phrasing patterns

\- Expected Output:

|<p>{</p><p>`  `"Label": "AI",</p><p>`  `"Confidence": "87",</p><p>`  `"Reasoning": {</p><p>`    `Explanation: "Repeated the phrase 'in conclusion' unnaturally.", </p><p>` `}</p><p>}</p>|
| :- |
|<p>{</p><p>`  `"Label": "Human",</p><p>`  `"Confidence": "74",</p><p>`  `"Reasoning": {</p><p>`    `" Explanation: "No unusual repetition, flow is natural, includes personal details about lived experiences.".</p><p>`  `}</p><p>}</p>|


1. ***Documentation on Prompt Testing and Refinement***

   - Design Initial Prompt: define role, evaluation criteria, output format (scalar or binary), and reasoning requirement.
   - Prompt Testing: run prompts and check for
- Adherence to format (number or label and reasoning?).
- Reasoning specificity (reference the criteria?).
- Consistency across outputs (similar-level feedback gets similar scores).
  - Reliability Check: check score, iterate, and rating.
  - Refinement Techniques:
- Test with zero-shot prompting: work well for generalized queries, fast response generation.
- Test with few-shot prompting: helps AI understand user intent better, improves contextual accuracy.
- Test with multiple response sampling: enhances reliability by selecting the most consistent answer.
- Direct Instruction Prompting: give clear and precise responses, reduces ambiguity 
  - Validation check:
- Compare AI-generated ratings using correlation (such as Pearson/Spearman) and agreement (such as Cohen's kappa) metrics.
- If correlation high (more than 0.7), finalize prompt. Otherwise, refine further.

\- Recourse: Rao S. M & Prasuna VG (2025), “Chain of Draft Prompting: Structured Approach to Efficient AI Reasoning”. Project: AAIE – Artificial Assessment Intelligence for Education

Week 6: Generative AI Rating Criteria Task

Activity: Model Development. [Planner - Generative AI Rating Criteria](https://teams.microsoft.com/l/entity/com.microsoft.teamspace.tab.planner/mytasks?tenantId=d02378ec-1688-46d5-8540-1c28b5f470f6&webUrl=https%3A%2F%2Ftasks.teams.microsoft.com%2Fteamsui%2FpersonalApp%2Falltasklists&context=%7B%22subEntityId%22%3A%22%2Fv1%2Fassignedtome%2Fview%2Fboard%2Ftask%2FGm-KKFL6cECuk7MdRUObD8gAL6Ck%22%7D)

**Objective:** 

- AI rating prompts for feedback generation task
- AI rating prompts for AI detection task
- Documentation on prompt testing and refinement

**Implementing:**

1. ***Hypothesis for Test Prompts:***

   - Primary Hypothesis: if the prompts are structured with explicit evaluation criteria, output formats, and reasoning requirements, then the generative AI will produce ratings that are meaningful and consistent

   - Meaningfulness (Validity): using metrics as Pearson/Spearman correlation, and Cohen's kappa agreement.
     - Threshold: Correlation >= 0.7 with human annotations.
   - Consistency (Reliability): run identical or paraphrased model outputs multiple times. 
     - Ratings should remain stable (variance <= 0.5 on a 1-5 scale).

1. ***AI Rating Prompts for Feedback Generation Task (Rating + Reasoning):***

   - Prompt evaluation should include explanation: using LLM-based judge that not only rates outputs (pass/fail or scalar rating) but also provides reasoning based on evaluation criteria which gives insight into why a response passes or fails.

   - Evaluation Criteria: rate a student-feedback response produced by an AI model, based on evaluation criteria.
   - Format:
     - Rating (Integer): score from 1 to 5 (from poor/not relevant to excellent/very relevant)
     - Reasoning:
   - Correctness – Is the feedback accurate and helpful?
   - Clarity – Is it easy to understand?
   - Tone – Is it supportive and constructive?
   - Actionability – Does it suggest a clear next step?

(3) AI Feedback Generation

\- Example:

|<p># Rate the relevance of the following response to the prompt: </p><p>Your Task: "Score this response from 1 (not relevant) to 5 (very relevant) and provide reasons for your score." </p><p></p><p>- Prompt1 = "What are the benefits of regular exercise?"</p><p>- Response = "Regular exercise improves cardiovascular health, boosts mood, and enhances overall fitness." </p><p></p><p># Evaluate clarity instead: </p><p>- Prompt2 = "How clear is this explanation? Score it from 1 (very unclear) to 5 (very clear)." </p><p>- Response = "Exercise helps you stay healthy by making your heart stronger and improving your mood."</p>|
| :- |

\- Expected Output: JSON format:

|<p>{</p><p>`  `"Rating": "<integer 1-5>",</p><p>`  `"Criterion\_Ratings": {</p><p>`    `"Correctness": "<1-5>",</p><p>`    `"Clarity": "<1-5>",</p><p>`    `"Tone": "<1-5>",</p><p>`    `"Actionability": "<1-5>"</p><p>},</p><p>`  `"Reasoning": {</p><p>`    `"Correctness": "Feedback correctly identified the logical error in the code.",</p><p>`    `"Clarity": "The explanation was clear and easy to follow.",</p><p>`    `"Tone": "Supportive and encouraging tone was maintained.",</p><p>`    `"Actionability": "Concrete next steps were suggested to fix the bug."</p><p>`  `}}</p>|
| :- |

\- References: Bakst W (03 March 2025), “Prompt Evaluation – Method, Tools, and Best Practices”. <https://mirascope.com/blog/prompt-evaluation>


1. ***AI Rating Prompts for AI Detection Task (Binary Output)***

   - Determine whether the following text was generated by AI or Human Written

   - Label (Output): AI or Human
   - Reasoning (Explanation): provide a brief reasoning (short sentence) such as cites, stylistic clues, or phrasing patterns

\- Expected Output:

|<p>{</p><p>`  `"Label": "AI",</p><p>`  `"Confidence": "87",</p><p>`  `"Reasoning": {</p><p>`    `Explanation: "Repeated the phrase 'in conclusion' unnaturally.", </p><p>` `}</p><p>}</p>|
| :- |
|<p>{</p><p>`  `"Label": "Human",</p><p>`  `"Confidence": "74",</p><p>`  `"Reasoning": {</p><p>`    `" Explanation: "No unusual repetition, flow is natural, includes personal details about lived experiences.".</p><p>`  `}</p><p>}</p>|


1. ***Documentation on Prompt Testing and Refinement***

   - Design Initial Prompt: define role, evaluation criteria, output format (scalar or binary), and reasoning requirement.
   - Prompt Testing: run prompts and check for
- Adherence to format (number or label and reasoning?).
- Reasoning specificity (reference the criteria?).
- Consistency across outputs (similar-level feedback gets similar scores).
  - Reliability Check: check score, iterate, and rating.
  - Refinement Techniques:
- Test with zero-shot prompting: work well for generalized queries, fast response generation.
- Test with few-shot prompting: helps AI understand user intent better, improves contextual accuracy.
- Test with multiple response sampling: enhances reliability by selecting the most consistent answer.
- Direct Instruction Prompting: give clear and precise responses, reduces ambiguity 
  - Validation check:
- Compare AI-generated ratings using correlation (such as Pearson/Spearman) and agreement (such as Cohen's kappa) metrics.
- If correlation high (more than 0.7), finalize prompt. Otherwise, refine further.

\- Recourse: Rao S. M & Prasuna VG (2025), “Chain of Draft Prompting: Structured Approach to Efficient AI Reasoning”. [https://www.ijfmr.com/papers/2025/3/46661.pdf?](https://www.ijfmr.com/papers/2025/3/46661.pdf?utm_source=chatgpt.com) 

