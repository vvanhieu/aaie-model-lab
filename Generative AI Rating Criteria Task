Project: AAIE – Artificial Assessment Intelligence for Education
Week 6: Generative AI Rating Criteria Task
Activity: Model Development. Planner - Generative AI Rating Criteria

Objective: 
•	AI rating prompts for feedback generation task
•	AI rating prompts for AI detection task
•	Documentation on prompt testing and refinement
Implementing:

1.	Hypothesis for Test Prompts:

- Primary Hypothesis: if the prompts are structured with explicit evaluation criteria, output formats, and reasoning requirements, then the generative AI will produce ratings that are meaningful and consistent
o	Meaningfulness (Validity): using metrics as Pearson/Spearman correlation, and Cohen's kappa agreement.
	Threshold: Correlation >= 0.7 with human annotations.
o	Consistency (Reliability): run identical or paraphrased model outputs multiple times. 
	Ratings should remain stable (variance <= 0.5 on a 1-5 scale).

2.	AI Rating Prompts for Feedback Generation Task (Rating + Reasoning):

- Prompt evaluation should include explanation: using LLM-based judge that not only rates outputs (pass/fail or scalar rating) but also provides reasoning based on evaluation criteria which gives insight into why a response passes or fails.

a)	Evaluation Criteria: rate a student-feedback response produced by an AI model, based on evaluation criteria.
b)	Format:
(1)	Rating (Integer): score from 1 to 5 (from poor/not relevant to excellent/very relevant)
(2)	Reasoning:
	Correctness – Is the feedback accurate and helpful?
	Clarity – Is it easy to understand?
	Tone – Is it supportive and constructive?
	Actionability – Does it suggest a clear next step?
(3) AI Feedback Generation

- Example:

# Rate the relevance of the following response to the prompt: 
Your Task: "Score this response from 1 (not relevant) to 5 (very relevant) and provide reasons for your score." 

•	Prompt1 = "What are the benefits of regular exercise?"
•	Response = "Regular exercise improves cardiovascular health, boosts mood, and enhances overall fitness." 

# Evaluate clarity instead: 
•	Prompt2 = "How clear is this explanation? Score it from 1 (very unclear) to 5 (very clear)." 
•	Response = "Exercise helps you stay healthy by making your heart stronger and improving your mood."

- Expected Output: JSON format:

{
  "Rating": "<integer 1-5>",
  "Criterion_Ratings": {
    "Correctness": "<1-5>",
    "Clarity": "<1-5>",
    "Tone": "<1-5>",
    "Actionability": "<1-5>"
},
  "Reasoning": {
    "Correctness": "Feedback correctly identified the logical error in the code.",
    "Clarity": "The explanation was clear and easy to follow.",
    "Tone": "Supportive and encouraging tone was maintained.",
    "Actionability": "Concrete next steps were suggested to fix the bug."
  }}

- References: Bakst W (03 March 2025), “Prompt Evaluation – Method, Tools, and Best Practices”. https://mirascope.com/blog/prompt-evaluation


3.	AI Rating Prompts for AI Detection Task (Binary Output)

- Determine whether the following text was generated by AI or Human Written
a)	Label (Output): AI or Human
b)	Reasoning (Explanation): provide a brief reasoning (short sentence) such as cites, stylistic clues, or phrasing patterns

- Expected Output:

{
  "Label": "AI",
  "Confidence": "87",
  "Reasoning": {
    Explanation: "Repeated the phrase 'in conclusion' unnaturally.", 
 }
}
{
  "Label": "Human",
  "Confidence": "74",
  "Reasoning": {
    " Explanation: "No unusual repetition, flow is natural, includes personal details about lived experiences.".
  }
}


4.	Documentation on Prompt Testing and Refinement

(1)	Design Initial Prompt: define role, evaluation criteria, output format (scalar or binary), and reasoning requirement.
(2)	Prompt Testing: run prompts and check for
	Adherence to format (number or label and reasoning?).
	Reasoning specificity (reference the criteria?).
	Consistency across outputs (similar-level feedback gets similar scores).
(3)	Reliability Check: check score, iterate, and rating.
(4)	Refinement Techniques:
	Test with zero-shot prompting: work well for generalized queries, fast response generation.
	Test with few-shot prompting: helps AI understand user intent better, improves contextual accuracy.
	Test with multiple response sampling: enhances reliability by selecting the most consistent answer.
	Direct Instruction Prompting: give clear and precise responses, reduces ambiguity 
(5)	Validation check:
	Compare AI-generated ratings using correlation (such as Pearson/Spearman) and agreement (such as Cohen's kappa) metrics.
	If correlation high (more than 0.7), finalize prompt. Otherwise, refine further.

- Recourse: Rao S. M & Prasuna VG (2025), “Chain of Draft Prompting: Structured Approach to Efficient AI Reasoning”. https://www.ijfmr.com/papers/2025/3/46661.pdf?utm_source=chatgpt.com
